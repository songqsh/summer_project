{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solver_pytorch_v03.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wqiu96/summer_project/blob/master/DeepBSDE_pytorch/solver_pytorch_v03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73bJqAeGufsI",
        "colab_type": "code",
        "outputId": "850c35bb-b8e9-4b31-f390-65d7ed99c641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "!git clone https://github.com/wqiu96/summer_project.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'summer_project'...\n",
            "remote: Enumerating objects: 157, done.\u001b[K\n",
            "remote: Counting objects: 100% (157/157), done.\u001b[K\n",
            "remote: Compressing objects: 100% (157/157), done.\u001b[K\n",
            "remote: Total 625 (delta 66), reused 0 (delta 0), pack-reused 468\u001b[K\n",
            "Receiving objects: 100% (625/625), 2.53 MiB | 18.94 MiB/s, done.\n",
            "Resolving deltas: 100% (314/314), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVY1CtJ9utse",
        "colab_type": "code",
        "outputId": "a1dad462-b8d4-44f6-bf86-d806a90e4d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd summer_project/DeepBSDE_pytorch/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/summer_project/DeepBSDE_pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vvaZbEWRYSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import uniform\n",
        "import torchvision\n",
        "from equation_pytorch import get_equation\n",
        "from config_pytorch import get_config\n",
        "\n",
        "\n",
        "MOMENTUM = 0.99\n",
        "EPSILON = 1e-6\n",
        "DELTA_CLIP = 50.0\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,num_hiddens):\n",
        "      super(Net, self).__init__()\n",
        "      self.num_hiddens = num_hiddens\n",
        "      \n",
        "      self.fc1 = nn.Linear(num_hiddens[0], num_hiddens[1], bias=False)\n",
        "      self.norm1 = nn.LayerNorm(num_hiddens[1])\n",
        "      self.fc2 = nn.Linear(num_hiddens[1], num_hiddens[2], bias=False)\n",
        "      self.norm2 = nn.LayerNorm(num_hiddens[2])\n",
        "      self.fc3 = nn.Linear(num_hiddens[2], num_hiddens[3], bias=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      # h1 = relu(xw1)\n",
        "      x = self.norm1(F.relu(self.fc1(x)))\n",
        "      # h2 = relu(h1w2)\n",
        "      x = self.norm1(F.relu(self.fc2(x)))\n",
        "      # h3 = h2w3\n",
        "      x = self.fc3(x)\n",
        "      #termin time\n",
        "      return x\n",
        "      \n",
        "\n",
        "class DeepNet(nn.Module):\n",
        "    def __init__(self,num_hiddens,config,bsde):\n",
        "      super(DeepNet, self).__init__()\n",
        "      self.num_hiddens = num_hiddens\n",
        "      self._config = config\n",
        "      self._bsde = bsde\n",
        "      self.y_init = 0\n",
        "      # make sure consistent with FBSDE equation\n",
        "      self._dim = bsde.dim\n",
        "      self._num_time_interval = bsde.num_time_interval\n",
        "      # ops for statistics update of batch normalization\n",
        "      self.linears = nn.ModuleList([Net(num_hiddens) for i in range(bsde.num_time_interval - 1)])\n",
        "      m = uniform.Uniform(config.y_init_range[0],config.y_init_range[1])\n",
        "      self.y_init = torch.nn.Parameter(m.sample())\n",
        "      #self.register_param()\n",
        "      \n",
        "   # def register_param(self):\n",
        "  #      exist_w = hasattr(self.module, 'y_init')\n",
        "   #     if not exist_w:\n",
        "   #       m = uniform.Uniform(config.y_init_range[0],config.y_init_range[1])\n",
        "   #       y_init = nn.Parameter(m.sample())\n",
        "   #       self.register_parameter(y_init) # register 'w' to module\n",
        "    \n",
        "    def forward(self,x):\n",
        "      #dw_train= torch.from_numpy(self._bsde.sample()[0])\n",
        "      dw_train= self._bsde.sample()[0].astype(np.float32)\n",
        "      time_stamp = np.arange(0, self._bsde.num_time_interval) * self._bsde.delta_t\n",
        "      z = 2*torch.rand([self._dim,1],dtype=torch.float32) - 1 #same as the original\n",
        "      y_ = self.y_init\n",
        "      for t in range(0,bsde.num_time_interval - 1):\n",
        "        dw = torch.from_numpy(dw_train[:, t]).view(1,self._dim)\n",
        "        torch.mm(dw.float(), z.float()) # torch.mm have bug use x.float()\n",
        "        y_ = y_ - self._bsde.delta_t* (self._bsde.f_tf(time_stamp[t], x[:, t], y_, z)) + torch.mm(dw, z)\n",
        "        z = (self.linears[t](x[:,t]) / self._dim).view(self._dim,1)\n",
        "      #terminal condition\n",
        "      dw = torch.from_numpy(dw_train[:, -1]).view(1,self._dim)\n",
        "      y_ = y_ - self._bsde.delta_t * (self._bsde.f_tf(time_stamp[-1], x[:, -2], y_, z)) + torch.mm(dw, z)\n",
        "      return y_\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T2dlXoInZEV",
        "colab_type": "text"
      },
      "source": [
        "Result:\n",
        "- AllenCahn: sometimes my code can give the similar result as the original code, sometimes the result is nan(because the loss is too small to learn??).\n",
        "- HJB: same as the original\n",
        "- PricingOption: same as or better than the original(The original code does not have a break)\n",
        "- PricingDefaultRisk: same as or better than the original(The original code does not have a break)\n",
        "- BurgesType: same as or better than the original(The original code does not have a break)\n",
        "- QuadraticGradients: same as or better than the original(The original code does not have a break)\n",
        "- eactionDiffusion : same as or better than the original(maybe, because loss is too late)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsW-av2BdFQZ",
        "colab_type": "code",
        "outputId": "4e521651-f6a5-4a75-bb67-c04a7d587932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "source": [
        "config = get_config('HJB')\n",
        "bsde = get_equation('HJB', config.dim, config.total_time, config.num_time_interval)\n",
        "\n",
        "deepNet = DeepNet(config.num_hiddens,config,bsde)\n",
        "optimizer = optim.SGD(deepNet.parameters(), lr=0.0000001, momentum=MOMENTUM) # lr have some different wiht the original\n",
        "#torch.optim.lr_scheduler.MultiStepLR(optimizer, [15,25,35], gamma=0.1, last_epoch=-1) # Adjust learning rate according to time\n",
        "train_loss = []\n",
        "for epoch in range(10000):\n",
        "  x_ = bsde.sample()[1].astype(np.float32)\n",
        "  x = torch.from_numpy(x_)\n",
        "  out = deepNet(x)\n",
        "  delta = out - bsde.g_tf(bsde.total_time, x[:, -1])\n",
        "  loss = torch.mean(torch.where(torch.abs(delta) < DELTA_CLIP, torch.pow(delta,2),2 * DELTA_CLIP * torch.abs(delta) - DELTA_CLIP ** 2))\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  train_loss.append(loss.item())\n",
        "  if epoch % 100 == 0 :\n",
        "    print(epoch, loss.item(), deepNet.y_init,out)\n",
        "  if 1000*loss <= 0.0001:\n",
        "    print(epoch, loss.item(), deepNet.y_init,out)\n",
        "    break\n",
        "    "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.4980943202972412 Parameter containing:\n",
            "tensor(0.7549, requires_grad=True) tensor([[1.0443]], grad_fn=<AddBackward0>)\n",
            "100 0.026739289984107018 Parameter containing:\n",
            "tensor(0.7547, requires_grad=True) tensor([[0.5727]], grad_fn=<AddBackward0>)\n",
            "200 0.37423697113990784 Parameter containing:\n",
            "tensor(0.7547, requires_grad=True) tensor([[1.5609]], grad_fn=<AddBackward0>)\n",
            "300 1.0942531824111938 Parameter containing:\n",
            "tensor(0.7549, requires_grad=True) tensor([[1.8347]], grad_fn=<AddBackward0>)\n",
            "400 0.10216192156076431 Parameter containing:\n",
            "tensor(0.7552, requires_grad=True) tensor([[-0.0401]], grad_fn=<AddBackward0>)\n",
            "500 0.8812061548233032 Parameter containing:\n",
            "tensor(0.7555, requires_grad=True) tensor([[1.3443]], grad_fn=<AddBackward0>)\n",
            "600 5.90281343460083 Parameter containing:\n",
            "tensor(0.7555, requires_grad=True) tensor([[-0.1688]], grad_fn=<AddBackward0>)\n",
            "700 0.678302526473999 Parameter containing:\n",
            "tensor(0.7558, requires_grad=True) tensor([[0.9748]], grad_fn=<AddBackward0>)\n",
            "800 0.6817721724510193 Parameter containing:\n",
            "tensor(0.7560, requires_grad=True) tensor([[-0.2579]], grad_fn=<AddBackward0>)\n",
            "900 0.08818832039833069 Parameter containing:\n",
            "tensor(0.7562, requires_grad=True) tensor([[0.6843]], grad_fn=<AddBackward0>)\n",
            "1000 0.9258301854133606 Parameter containing:\n",
            "tensor(0.7562, requires_grad=True) tensor([[1.3924]], grad_fn=<AddBackward0>)\n",
            "1100 0.022190535441040993 Parameter containing:\n",
            "tensor(0.7564, requires_grad=True) tensor([[0.2206]], grad_fn=<AddBackward0>)\n",
            "1200 0.10680563747882843 Parameter containing:\n",
            "tensor(0.7568, requires_grad=True) tensor([[1.5960]], grad_fn=<AddBackward0>)\n",
            "1300 0.35752108693122864 Parameter containing:\n",
            "tensor(0.7570, requires_grad=True) tensor([[1.2630]], grad_fn=<AddBackward0>)\n",
            "1400 0.47736912965774536 Parameter containing:\n",
            "tensor(0.7571, requires_grad=True) tensor([[0.7035]], grad_fn=<AddBackward0>)\n",
            "1500 0.12511633336544037 Parameter containing:\n",
            "tensor(0.7573, requires_grad=True) tensor([[0.8570]], grad_fn=<AddBackward0>)\n",
            "1600 0.06395377218723297 Parameter containing:\n",
            "tensor(0.7575, requires_grad=True) tensor([[2.2475]], grad_fn=<AddBackward0>)\n",
            "1700 9.793233871459961 Parameter containing:\n",
            "tensor(0.7579, requires_grad=True) tensor([[1.4704]], grad_fn=<AddBackward0>)\n",
            "1761 1.2238160707056522e-08 Parameter containing:\n",
            "tensor(0.7582, requires_grad=True) tensor([[0.4185]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}